#! /bin/bash

set -e -x

sudo -E -u hdfs hadoop fs -mkdir -p /user/hive/warehouse
sudo -E -u hdfs hadoop fs -chmod a+w /user/hive/warehouse
sudo -E -u hdfs hadoop fs -chmod a+w /tmp

cd /home/hive
sudo -E -u hive nohup ${HIVE_HOME}/bin/hiveserver2 > /home/hive/hive.log & sleep 15

cd /home/jenkins
# Download dataset and upload it to HDFS
wget https://s3.amazonaws.com/h2o-public-test-data/smalldata/airlines/AirlinesTest.csv.zip
unzip AirlinesTest.csv.zip
rm AirlinesTest.csv.zip
sed -i 's/\"//g' AirlinesTest.csv

sudo -E -u hive hadoop fs -put -f ./AirlinesTest.csv /tmp/AirlinesTest.csv

# Execute all hive-scripts
cd /opt/hive-scripts
for f in $(ls); do
    sudo -E -u hive ${HIVE_HOME}/bin/beeline -u jdbc:hive2://localhost:10000 -f ${f}
done

sudo -E -u hive hadoop fs -put -f ./AirlinesTest.csv /tmp/AirlinesTest.csv
rm AirlinesTest.csv